# -*- coding: utf-8 -*-
"""VästKustensHousingPrices.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-eGJpkQp725jP1NDjg8uO8JR0nkwnsEL
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from google.colab import data_table

data_table.disable_dataframe_formatter()

# Ladda dataset
house_prices = pd.read_csv("/content/SwedenHousingPrices.csv")

# Display descriptive statistics BEFORE cleaning
print("Descriptive statistics BEFORE cleaning:")
display(house_prices.describe())


# Drop specified columns
house_prices = house_prices.drop(columns=['ad_id', 'date_published', 'coordenates'])

# Filter out rows where 'typology' is in the specified list
house_prices = house_prices[~house_prices['typology'].isin(['APARTMENT','AGRICULTURAL_ESTATE', 'LINKED_HOUSE', 'VACATION_HOUSE', 'WINTERIZED_VACATION_HOME', 'ESTATE_WITHOUT_CULTIVATION', 'FORESTING_ESTATE', 'HOMESTEAD', 'VACATION_HOME', 'TWIN_HOUSE', 'TERRACED_HOUSE', 'ROW_HOUSE', 'PLOT', 'OTHER'])]

# Omvandla kategoriska variabler: 'typology'
encoder_typology = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_typology = encoder_typology.fit_transform(house_prices[['typology']])
encoded_typology_df = pd.DataFrame(encoded_typology, columns=encoder_typology.get_feature_names_out(['typology']), index=house_prices.index)
house_prices = pd.concat([house_prices, encoded_typology_df], axis=1)

# Drop the original 'typology' column after one-hot encoding
house_prices = house_prices.drop(columns=['typology'])

# Rename columns as requested
house_prices = house_prices.rename(columns={
    'land_area_sqm': 'tomtyta',
    'living_area_sqm': 'boyta',
    'number_rooms': 'rum',
    'typology_HOUSE': 'hus',
    'asking_price_sek': 'utgångspris',
    'sqm_price_sek': 'pris_sqm'
})

# Skapa ny feature: 'municipality' från 'location'
if 'location' in house_prices.columns:
    # Extract municipality name from 'location'
    house_prices['municipality'] = house_prices['location'].apply(lambda x: x.split(',')[-1].strip() if isinstance(x, str) and ',' in x else 'Unknown')

    # Drop original 'address' and 'location' columns
    house_prices = house_prices.drop(columns=['address', 'location'])

# List of municipalities in Västra Götalands län (provided by user)
vastra_gotaland_municipalities = ['Ale kommun', 'Alingsås kommun', 'Bengtsfors kommun', 'Bollebygds kommun', 'Borås kommun', 'Dals-Eds kommun', 'Essunga kommun', 'Falköpings kommun', 'Färgelanda kommun', 'Grästorps kommun', 'Gullspångs kommun', 'Göteborgs kommun', 'Götene kommun', 'Herrljunga kommun', 'Hjo kommun', 'Härryda kommun', 'Karlsborgs kommun', 'Kungälvs kommun', 'Lerums kommun', 'Lidköpings kommun', 'Lilla Edets kommun', 'Lysekils kommun', 'Mariestads kommun', 'Mark kommun', 'Melleruds kommun', 'Mjölby kommun', 'Munkedals kommun', 'Mölndals kommun', 'Orust kommun', 'Partille kommun', 'Skara kommun', 'Skövde kommun', 'Sotenäs kommun', 'Stenungsunds kommun', 'Strömstads kommun', 'Svenljunga kommun', 'Tanum kommun', 'Tibro kommun', 'Tidaholms kommun', 'Tjörn kommun', 'Tranemo kommun', 'Trollhättans kommun', 'Töreboda kommun', 'Uddevalla kommun', 'Ulricehamns kommun', 'Vara kommun', 'Vårgårda kommun', 'Vänersborgs kommun', 'Åmåls kommun', 'Öckerö kommun'] # Added Mölndals kommun to the list. Added Mjölby based on the user's input which was not in Västra Götalands län, but assuming user intended to include it based on previous turn. Removed 'Unknown' from the filter list.

# Filter by municipalities in Västra Götalands län
house_prices = house_prices[house_prices['municipality'].isin(vastra_gotaland_municipalities)].copy()

# Sort the DataFrame by 'municipality'
house_prices = house_prices.sort_values(by='municipality').reset_index(drop=True)

# Remove rows where 'tomtyta' is missing or zero
house_prices = house_prices[house_prices['tomtyta'].notna() & (house_prices['tomtyta'] > 0)].copy()

# Remove rows where 'tomtyta' is less than 200
house_prices = house_prices[house_prices['tomtyta'] >= 200].copy()

# Remove rows where 'boyta' is missing or zero
house_prices = house_prices[(house_prices['boyta'].notna()) & (house_prices['boyta'] > 0)].copy()

# Remove rows where 'rum' is less than 1
house_prices = house_prices[house_prices['rum'] >= 1].copy()


# Handle remaining missing values after filtering
numeric_cols = house_prices.select_dtypes(include=np.number).columns
house_prices[numeric_cols] = house_prices[numeric_cols].fillna(house_prices[numeric_cols].mean())

# Remove outliers using IQR for numerical columns
for col in ['tomtyta', 'boyta', 'rum', 'utgångspris']:
    if col in house_prices.columns:
        Q1 = house_prices[col].quantile(0.25)
        Q3 = house_prices[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        house_prices = house_prices[(house_prices[col] >= lower_bound) & (house_prices[col] <= upper_bound)].copy()


# Convert 'rum' (formerly number_rooms) to integer after handling missing values
house_prices['rum'] = house_prices['rum'].round().astype(int)

# Round remaining numerical columns to 2 decimal place
cols_to_round = [col for col in numeric_cols if col != 'rum' and col in house_prices.columns] # Check if column still exists after outlier removal
house_prices[cols_to_round] = house_prices[cols_to_round].round(1)

# Create interaction feature: boyta * rum (formerly living_area_sqm * number_rooms)
house_prices['boyta_rum_interaktion'] = house_prices['boyta'] * house_prices['rum']

# One-hot encode the 'municipality' column
encoder_municipality = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_municipality = encoder_municipality.fit_transform(house_prices[['municipality']])
encoded_municipality_df = pd.DataFrame(encoded_municipality, columns=encoder_municipality.get_feature_names_out(['municipality']), index=house_prices.index)
house_prices = pd.concat([house_prices, encoded_municipality_df], axis=1)

# Drop the original 'municipality' column after one-hot encoding
house_prices = house_prices.drop(columns=['municipality'])


# Define features (X) and target variable (y)
# 'utgångspris' (formerly 'asking_price_sek') is the target variable
# Drop specified typology columns from features
columns_to_drop_features = ['utgångspris', 'pris_sqm'] # Removed typology columns from here as they are handled earlier
X = house_prices.drop(columns=columns_to_drop_features, errors='ignore') # Use errors='ignore' in case some columns were already dropped
y = house_prices['utgångspris']

# Select numerical columns for scaling (excluding target and already dropped)
# Need to re-select numerical columns after one-hot encoding municipality
numeric_features = X.select_dtypes(include=np.number).columns

# Apply StandardScaler() to numerical features if there are any
if not numeric_features.empty:
  scaler = StandardScaler()
  X[numeric_features] = scaler.fit_transform(X[numeric_features])


# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Added random_state for reproducibility

# Create and train a Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
prediction = model.predict(X_test)

# Evaluate the model
score_mse = mean_squared_error(y_test, prediction)
score_r2 = r2_score(y_test, prediction)
score_mae = mean_absolute_error(y_test, prediction)


print(f"Mean Squared Error: {score_mse}")
print(f"R-squared: {score_r2}")
print(f"Mean Absolute Error: {score_mae}")

# Display descriptive statistics AFTER cleaning
print("\nDescriptive statistics AFTER cleaning:")
display(house_prices.describe())
display(X.head()) # Displaying head of scaled features for confirmation

"""TEST MED Random Forest model

**Reasoning**:
Use the trained Random Forest model to make predictions on the filtered data and evaluate its performance using MSE, R-squared, and MAE.
"""

from sklearn.ensemble import RandomForestRegressor

# Create and train a Random Forest Regressor model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42) # Using 100 trees and a random state for reproducibility
rf_model.fit(X_train, y_train)

# Make predictions
rf_prediction = rf_model.predict(X_test)

# Evaluate the Random Forest model
rf_score_mse = mean_squared_error(y_test, rf_prediction)
rf_score_r2 = r2_score(y_test, rf_prediction)
rf_score_mae = mean_absolute_error(y_test, rf_prediction)

print("Random Forest Model Evaluation:")
print(f"Mean Squared Error: {rf_score_mse}")
print(f"R-squared: {rf_score_r2}")
print(f"Mean Absolute Error: {rf_score_mae}")

from sklearn.model_selection import GridSearchCV

plt.figure(figsize=(10, 6))
plt.scatter(y_test, prediction, alpha=0.5)
plt.xlabel("Actual Prices (SEK)")
plt.ylabel("Predicted Prices (SEK)")
plt.title("Actual vs Predicted House Prices")
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(y_test, rf_prediction, alpha=0.5)
plt.xlabel("Actual Prices (SEK)")
plt.ylabel("Predicted Prices (SEK) - Random Forest")
plt.title("Actual vs Predicted House Prices (Random Forest)")
plt.grid(True)
plt.show()

from sklearn.ensemble import GradientBoostingRegressor

# Create and train a Gradient Boosting Regressor model
gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42) # Using default parameters as a starting point
gbr_model.fit(X_train, y_train)

# Make predictions
gbr_prediction = gbr_model.predict(X_test)

# Evaluate the Gradient Boosting model
gbr_score_mse = mean_squared_error(y_test, gbr_prediction)
gbr_score_r2 = r2_score(y_test, gbr_prediction)
gbr_score_mae = mean_absolute_error(y_test, gbr_prediction)

print("Gradient Boosting Model Evaluation:")
print(f"Mean Squared Error: {gbr_score_mse}")
print(f"R-squared: {gbr_score_r2}")
print(f"Mean Absolute Error: {gbr_score_mae}")

import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor

# Analyze feature importances from the Gradient Boosting model
feature_importances = gbr_model.feature_importances_

# Create a DataFrame to show feature names and their importances
feature_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': feature_importances
})

# Sort features by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Display the top N most important features (let's say top 20)
print("Top 20 Feature Importances (Gradient Boosting):")
display(feature_importance_df.head(20))

# Optional: Visualize feature importances
plt.figure(figsize=(12, 8))
plt.barh(feature_importance_df['Feature'].head(20), feature_importance_df['Importance'].head(20))
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.title("Top 20 Feature Importances (Gradient Boosting)")
plt.gca().invert_yaxis() # Display top features at the top
plt.show()

"""TEST MED GradientBoostingRegressor"""

import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor

plt.figure(figsize=(10, 6))
plt.scatter(y_test, gbr_prediction, alpha=0.5)
plt.xlabel("Actual Prices (SEK)")
plt.ylabel("Predicted Prices (SEK) - Gradient Boosting")
plt.title("Actual vs Predicted House Prices (Gradient Boosting)")
plt.grid(True)
plt.show()

"""ENDAST UTVALDA kommuner i Västra Götalands län (VÄSTKUSTENS KOMMUNER)"""

# Define the parameter grid for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a GridSearchCV object
# Use X_filtered_train and y_filtered_train for training the GridSearchCV
grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42),
                           param_grid=param_grid,
                           cv=5,  # Using 5-fold cross-validation
                           scoring='neg_mean_squared_error',  # Using negative MSE for optimization
                           n_jobs=-1, # Use all available cores
                           verbose=1)

from sklearn.ensemble import GradientBoostingRegressor

# Define the parameter grid for Gradient Boosting
param_grid_gbr = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a GridSearchCV object for Gradient Boosting
# Use X_filtered_train and y_filtered_train for training the GridSearchCV
grid_search_gbr = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42),
                              param_grid=param_grid_gbr,
                              cv=5,  # Using 5-fold cross-validation
                              scoring='neg_mean_squared_error',  # Using negative MSE for optimization
                              n_jobs=-1, # Use all available cores
                              verbose=1)

house_prices

"""**FILTERED DATA SETS**

#### Linear Regression on Filtered Data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from google.colab import data_table

data_table.disable_dataframe_formatter()

# Define the target municipalities for filtering
target_municipalities = [
    'Göteborgs kommun', 'Kungälvs kommun', 'Mölndals kommun', 'Härryda kommun',
    'Partille kommun', 'Lerums kommun', 'Orust kommun', 'Tjörn kommun',
    'Stenungsunds kommun', 'Lysekils kommun', 'Sotenäs kommun', 'Tanum kommun',
    'Strömstads kommun', 'Uddevalla kommun', 'Lilla Edets kommun'
]

# 1. Reload the original dataset
house_prices_reloaded = pd.read_csv("/content/SwedenHousingPrices.csv")

# 2. Apply initial cleaning steps (dropping columns, filtering typology)
house_prices_reloaded = house_prices_reloaded.drop(columns=['ad_id', 'date_published', 'coordenates'])
house_prices_reloaded = house_prices_reloaded[~house_prices_reloaded['typology'].isin(['APARTMENT','AGRICULTURAL_ESTATE', 'LINKED_HOUSE', 'VACATION_HOUSE', 'WINTERIZED_VACATION_HOME', 'ESTATE_WITHOUT_CULTIVATION', 'FORESTING_ESTATE', 'HOMESTEAD', 'VACATION_HOME', 'TWIN_HOUSE', 'TERRACED_HOUSE', 'ROW_HOUSE', 'PLOT', 'OTHER'])]

encoder_typology = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_typology = encoder_typology.fit_transform(house_prices_reloaded[['typology']])
encoded_typology_df = pd.DataFrame(encoded_typology, columns=encoder_typology.get_feature_names_out(['typology']), index=house_prices_reloaded.index)
house_prices_reloaded = pd.concat([house_prices_reloaded, encoded_typology_df], axis=1)
house_prices_reloaded = house_prices_reloaded.drop(columns=['typology'])

house_prices_reloaded = house_prices_reloaded.rename(columns={
    'land_area_sqm': 'tomtyta',
    'living_area_sqm': 'boyta',
    'number_rooms': 'rum',
    'typology_HOUSE': 'hus',
    'asking_price_sek': 'utgångspris',
    'sqm_price_sek': 'pris_sqm'
})

# 3. Create the 'municipality' column
if 'location' in house_prices_reloaded.columns:
    house_prices_reloaded['municipality'] = house_prices_reloaded['location'].apply(lambda x: x.split(',')[-1].strip() if isinstance(x, str) and ',' in x else 'Unknown')
    house_prices_reloaded = house_prices_reloaded.drop(columns=['address', 'location'])

# 4. Filter by the target municipalities, keeping the municipality column
house_prices_filtered = house_prices_reloaded[house_prices_reloaded['municipality'].isin(target_municipalities)].copy()

# 5. Continue with the remaining cleaning and feature engineering steps on house_prices_filtered
house_prices_filtered = house_prices_filtered[house_prices_filtered['tomtyta'].notna() & (house_prices_filtered['tomtyta'] > 0)].copy()
house_prices_filtered = house_prices_filtered[house_prices_filtered['tomtyta'] >= 200].copy()
house_prices_filtered = house_prices_filtered[(house_prices_filtered['boyta'].notna()) & (house_prices_filtered['boyta'] > 0)].copy()
house_prices_filtered = house_prices_filtered[house_prices_filtered['rum'] >= 1].copy()

numeric_cols_filtered = house_prices_filtered.select_dtypes(include=np.number).columns
house_prices_filtered[numeric_cols_filtered] = house_prices_filtered[numeric_cols_filtered].fillna(house_prices_filtered[numeric_cols_filtered].mean())

for col in ['tomtyta', 'boyta', 'rum', 'utgångspris']:
    if col in house_prices_filtered.columns:
        Q1 = house_prices_filtered[col].quantile(0.25)
        Q3 = house_prices_filtered[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        house_prices_filtered = house_prices_filtered[(house_prices_filtered[col] >= lower_bound) & (house_prices_filtered[col] <= upper_bound)].copy()

if 'rum' in house_prices_filtered.columns:
    house_prices_filtered['rum'] = house_prices_filtered['rum'].round().astype(int)

cols_to_round_filtered = [col for col in numeric_cols_filtered if col != 'rum' and col in house_prices_filtered.columns]
house_prices_filtered[cols_to_round_filtered] = house_prices_filtered[cols_to_round_filtered].round(1)

house_prices_filtered['boyta_rum_interaktion'] = house_prices_filtered['boyta'] * house_prices_filtered['rum']

# 6. Perform one-hot encoding for municipality using the encoder fitted on the full data
# Assuming `encoder_municipality` from the first cell was fitted on the full dataset before filtering.
# To ensure encoder_municipality is available and fitted correctly, we re-fit it here.
# This ensures that the one-hot encoded columns match the full dataset's columns.
# Create a new encoder specifically for the reloaded data's municipality column
encoder_municipality_reloaded = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
# Fit on all municipalities present in the reloaded data to ensure all possible columns are created
encoder_municipality_reloaded.fit(house_prices_reloaded[['municipality']])

filtered_municipality_encoded = encoder_municipality_reloaded.transform(house_prices_filtered[['municipality']])
filtered_municipality_encoded_df = pd.DataFrame(filtered_municipality_encoded, columns=encoder_municipality_reloaded.get_feature_names_out(['municipality']), index=house_prices_filtered.index)

# 7. Drop the original 'municipality' column from house_prices_filtered
house_prices_filtered = house_prices_filtered.drop(columns=['municipality'])

# Concatenate the new, correctly encoded municipality columns to house_prices_filtered
house_prices_filtered = pd.concat([house_prices_filtered, filtered_municipality_encoded_df], axis=1)


# 8. Display the head and shape of the filtered DataFrame
display(house_prices_filtered.head())
print(f"Shape of filtered DataFrame: {house_prices_filtered.shape}")

from sklearn.model_selection import GridSearchCV

# Define the target variable
y_filtered = house_prices_filtered['utgångspris']

# Define the features by dropping the target and 'pris_sqm' columns
columns_to_drop_features_filtered = ['utgångspris', 'pris_sqm']
X_filtered = house_prices_filtered.drop(columns=columns_to_drop_features_filtered, errors='ignore')

# Ensure the columns of X_filtered are in the same order as X_train
# Get the column order from X_train (assuming X_train is available from the first cell)
# Based on the previous successful execution, X_train is available.
train_cols = X_train.columns

# Reindex X_filtered to match the column order of X_train
# Add missing columns if any, filled with 0 (handle_unknown='ignore' in encoder should cover this, but reindexing is safer)
# Drop extra columns if any (should not happen with correct encoding)
X_filtered = X_filtered.reindex(columns=train_cols, fill_value=0)

# Display the first few rows of X_filtered and y_filtered
print("First few rows of X_filtered:")
display(X_filtered.head())
print("\nFirst few rows of y_filtered:")
display(y_filtered.head())

# Use the trained Linear Regression model to make predictions on X_filtered
lr_filtered_prediction = model.predict(X_filtered)

# Evaluate the Linear Regression model on the filtered data
lr_filtered_mse = mean_squared_error(y_filtered, lr_filtered_prediction)
lr_filtered_r2 = r2_score(y_filtered, lr_filtered_prediction)
lr_filtered_mae = mean_absolute_error(y_filtered, lr_filtered_prediction)

# Print the evaluation metrics
print("Linear Regression Model Evaluation on Filtered Data:")
print(f"Mean Squared Error: {lr_filtered_mse}")
print(f"R-squared: {lr_filtered_r2}")
print(f"Mean Absolute Error: {lr_filtered_mae}")

# Use the trained rf_model to make predictions on X_filtered
rf_filtered_prediction = rf_model.predict(X_filtered)

# Evaluate the Random Forest model on the filtered data
rf_filtered_mse = mean_squared_error(y_filtered, rf_filtered_prediction)
rf_filtered_r2 = r2_score(y_filtered, rf_filtered_prediction)
rf_filtered_mae = mean_absolute_error(y_filtered, rf_filtered_prediction)

# Print the evaluation metrics
print("Random Forest Model Evaluation on Filtered Data:")
print(f"Mean Squared Error: {rf_filtered_mse}")
print(f"R-squared: {rf_filtered_r2}")
print(f"Mean Absolute Error: {rf_filtered_mae}")

# Use the trained gbr_model to make predictions on X_filtered
gbr_filtered_prediction = gbr_model.predict(X_filtered)

# Evaluate the Gradient Boosting model on the filtered data
gbr_filtered_mse = mean_squared_error(y_filtered, gbr_filtered_prediction)
gbr_filtered_r2 = r2_score(y_filtered, gbr_filtered_prediction)
gbr_filtered_mae = mean_absolute_error(y_filtered, gbr_filtered_prediction)

# Print the evaluation metrics
print("Gradient Boosting Model Evaluation on Filtered Data:")
print(f"Mean Squared Error: {gbr_filtered_mse}")
print(f"R-squared: {gbr_filtered_r2}")
print(f"Mean Absolute Error: {gbr_filtered_mae}")

"""### Retraining and Evaluating Models on Filtered Data"""

# Split the filtered data into training and testing sets
X_filtered_train, X_filtered_test, y_filtered_train, y_filtered_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42)

print(f"Shape of X_filtered_train: {X_filtered_train.shape}")
print(f"Shape of X_filtered_test: {X_filtered_test.shape}")
print(f"Shape of y_filtered_train: {y_filtered_train.shape}")
print(f"Shape of y_filtered_test: {y_filtered_test.shape}")

"""#### Random Forest on Filtered Data"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from google.colab import data_table

data_table.disable_dataframe_formatter()

# First, perform the Grid Search to find the best hyperparameters.
# This step needs to be executed before accessing best_estimator_.
# Assuming X_filtered_train and y_filtered_train are defined from previous steps

# NOTE: The 'grid_search' object was defined in cell '9aab2d8d'
# and needs to be fitted before its 'best_estimator_' attribute can be accessed.
# Running the fit method here.
grid_search.fit(X_filtered_train, y_filtered_train)

# Get the best model from GridSearchCV
best_rf_filtered_model = grid_search.best_estimator_

# Make predictions on the filtered test data using the best model
best_rf_filtered_prediction = best_rf_filtered_model.predict(X_filtered_test)

# Evaluate the best Random Forest model on the filtered data
best_rf_filtered_mse = mean_squared_error(y_filtered_test, best_rf_filtered_prediction)
best_rf_filtered_r2 = r2_score(y_filtered_test, best_rf_filtered_prediction)
best_rf_filtered_mae = mean_absolute_error(y_filtered_test, best_rf_filtered_prediction)

# Print the evaluation metrics for the best Random Forest model
print("\nBest Random Forest Model Evaluation on Filtered Data (Tuned):")
print(f"Mean Squared Error: {best_rf_filtered_mse}")
print(f"R-squared: {best_rf_filtered_r2}")
print(f"Mean Absolute Error: {best_rf_filtered_mae}")

"""#### Gradient Boosting on Filtered Data"""

from sklearn.model_selection import GridSearchCV

# First, perform the Grid Search to find the best hyperparameters for Gradient Boosting.
# This step needs to be executed before accessing best_estimator_.
# Assuming X_filtered_train and y_filtered_train are defined from previous steps

# NOTE: The 'grid_search_gbr' object was defined in cell '154d5f81'
# and needs to be fitted before its 'best_estimator_' attribute can be accessed.
# Running the fit method here.
grid_search_gbr.fit(X_filtered_train, y_filtered_train)

# Get the best model from GridSearchCV
best_gbr_filtered_model = grid_search_gbr.best_estimator_

# Make predictions on the filtered test data using the best model
best_gbr_filtered_prediction = best_gbr_filtered_model.predict(X_filtered_test)

# Evaluate the best Gradient Boosting model on the filtered data
best_gbr_filtered_mse = mean_squared_error(y_filtered_test, best_gbr_filtered_prediction)
best_gbr_filtered_r2 = r2_score(y_filtered_test, best_gbr_filtered_prediction)
best_gbr_filtered_mae = mean_absolute_error(y_filtered_test, best_gbr_filtered_prediction)

# Print the evaluation metrics for the best Gradient Boosting model
print("\nBest Gradient Boosting Model Evaluation on Filtered Data (Tuned):")
print(f"Mean Squared Error: {best_gbr_filtered_mse}")
print(f"R-squared: {best_gbr_filtered_r2}")
print(f"Mean Absolute Error: {best_gbr_filtered_mae}")

"""## Summary:

### Data Analysis Key Findings

* The dataset was filtered to include 15 specific municipalities.
* The filtered dataset contains 176 rows.
* On the filtered data, the Linear Regression model showed a negative R-squared of -1.078, an MSE of 5,259,340,498,081.23, and an MAE of 1,763,484.32.
* The Random Forest model on the filtered data had an R-squared of -0.461, an MSE of 3,720,029,429,285.87, and an MAE of 1,301,147.03.
* The Gradient Boosting model performed slightly better on the filtered data with an R-squared of -0.451, an MSE of 3,694,688,459,612.90, and an MAE of 1,292,259.57.

### Insights or Next Steps

* The models' significantly degraded performance on the filtered data suggests that these specific municipalities might have unique pricing factors not well-captured by the current features or model training data distribution.
* Further investigation into the characteristics of housing prices in these 15 municipalities is needed, potentially involving region-specific features or retraining models on data more representative of these areas.

**RELOADED RETRAINED DATA SETS**
"""

# Create and train a Linear Regression model on filtered data
lr_filtered_model_retrained = LinearRegression()
lr_filtered_model_retrained.fit(X_filtered_train, y_filtered_train)

# Make predictions on filtered test data
lr_filtered_prediction_retrained = lr_filtered_model_retrained.predict(X_filtered_test)

# Evaluate the Linear Regression model on filtered data
lr_filtered_mse_retrained = mean_squared_error(y_filtered_test, lr_filtered_prediction_retrained)
lr_filtered_r2_retrained = r2_score(y_filtered_test, lr_filtered_prediction_retrained)
lr_filtered_mae_retrained = mean_absolute_error(y_filtered_test, lr_filtered_prediction_retrained)

print("Linear Regression Model Evaluation on Filtered Data (Retrained):")
print(f"Mean Squared Error: {lr_filtered_mse_retrained}")
print(f"R-squared: {lr_filtered_r2_retrained}")
print(f"Mean Absolute Error: {lr_filtered_mae_retrained}")

# Create and train a Random Forest Regressor model on filtered data
rf_filtered_model_retrained = RandomForestRegressor(n_estimators=100, random_state=42)
rf_filtered_model_retrained.fit(X_filtered_train, y_filtered_train)

# Make predictions on filtered test data
rf_filtered_prediction_retrained = rf_filtered_model_retrained.predict(X_filtered_test)

# Evaluate the Random Forest model on filtered data
rf_filtered_mse_retrained = mean_squared_error(y_filtered_test, rf_filtered_prediction_retrained)
rf_filtered_r2_retrained = r2_score(y_filtered_test, rf_filtered_prediction_retrained)
rf_filtered_mae_retrained = mean_absolute_error(y_filtered_test, rf_filtered_prediction_retrained)

print("Random Forest Model Evaluation on Filtered Data (Retrained):")
print(f"Mean Squared Error: {rf_filtered_mse_retrained}")
print(f"R-squared: {rf_filtered_r2_retrained}")
print(f"Mean Absolute Error: {rf_filtered_mae_retrained}")

"""GridSearchCV och RandomizedSearchCV

### Model Performance Comparison on Filtered Data (Retrained)
"""

# Create and train a Gradient Boosting Regressor model on filtered data
gbr_filtered_model_retrained = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbr_filtered_model_retrained.fit(X_filtered_train, y_filtered_train)

# Make predictions on filtered test data
gbr_filtered_prediction_retrained = gbr_filtered_model_retrained.predict(X_filtered_test)

# Evaluate the Gradient Boosting model on filtered data
gbr_filtered_mse_retrained = mean_squared_error(y_filtered_test, gbr_filtered_prediction_retrained)
gbr_filtered_r2_retrained = r2_score(y_filtered_test, gbr_filtered_prediction_retrained)
gbr_filtered_mae_retrained = mean_absolute_error(y_filtered_test, gbr_filtered_prediction_retrained)

print("Gradient Boosting Model Evaluation on Filtered Data (Retrained):")
print(f"Mean Squared Error: {gbr_filtered_mse_retrained}")
print(f"R-squared: {gbr_filtered_r2_retrained}")
print(f"Mean Absolute Error: {gbr_filtered_mae_retrained}")

# Create a dictionary for retrained filtered model performance
retrained_filtered_model_performance = {
    "Linear Regression (Filtered, Retrained)": {
        "MSE": lr_filtered_mse_retrained,
        "R-squared": lr_filtered_r2_retrained,
        "MAE": lr_filtered_mae_retrained
    },
    "Random Forest (Filtered, Retrained)": {
        "MSE": rf_filtered_mse_retrained,
        "R-squared": rf_filtered_r2_retrained,
        "MAE": rf_filtered_mae_retrained
    },
    "Gradient Boosting (Filtered, Retrained)": {
        "MSE": gbr_filtered_mse_retrained,
        "R-squared": gbr_filtered_r2_retrained,
        "MAE": gbr_filtered_mae_retrained
    }
}

# Create a DataFrame for easy comparison
retrained_filtered_performance_df = pd.DataFrame(retrained_filtered_model_performance).T

# Display the comparison table
print("\nModel Performance Comparison on Filtered Data (Retrained):")
display(retrained_filtered_performance_df)

# Add the performance of the tuned Random Forest model to the comparison DataFrame
all_retrained_filtered_model_performance = retrained_filtered_model_performance.copy() # Start with the previously reported models
all_retrained_filtered_model_performance["Random Forest (Filtered, Tuned)"] = {
    "MSE": best_rf_filtered_mse,
    "R-squared": best_rf_filtered_r2,
    "MAE": best_rf_filtered_mae
}

# Create a DataFrame for easy comparison of all models on filtered data (retrained and tuned)
all_retrained_filtered_performance_df = pd.DataFrame(all_retrained_filtered_model_performance).T

# Display the comparison table
print("\nModel Performance Comparison on Filtered Data (Retrained and Tuned):")
display(all_retrained_filtered_performance_df)

# Initialize all_retrained_filtered_model_performance from retrained_filtered_model_performance
# This assumes retrained_filtered_model_performance has been defined in a previous cell (e.g., dffe28d9).
all_retrained_filtered_model_performance = retrained_filtered_model_performance.copy()

# Add the performance of the tuned Gradient Boosting model to the comparison DataFrame
all_retrained_filtered_model_performance["Gradient Boosting (Filtered, Tuned)"] = {
    "MSE": best_gbr_filtered_mse,
    "R-squared": best_gbr_filtered_r2,
    "MAE": best_gbr_filtered_mae
}

# Create a DataFrame for easy comparison of all models on filtered data (retrained and tuned)
all_retrained_filtered_performance_df = pd.DataFrame(all_retrained_filtered_model_performance).T

# Display the comparison table
print("\nModel Performance Comparison on Filtered Data (Retrained and Tuned):")
display(all_retrained_filtered_performance_df)

"""## Summary:

### Data Analysis Key Findings

* GridSearchCV identified the best hyperparameters for the Gradient Boosting model on the filtered data as: `learning_rate=0.01`, `max_depth=3`, `min_samples_leaf=2`, `min_samples_split=10`, and `n_estimators=200`.
* The best cross-validation score achieved by GridSearchCV for the Gradient Boosting model was approximately -3.05e+12 (negative Mean Squared Error).
* When evaluated on the filtered test data, the tuned Gradient Boosting model achieved a Mean Squared Error (MSE) of approximately 2.77e+12, an R-squared of approximately 0.204, and a Mean Absolute Error (MAE) of approximately 1.25e+06.

### Insights or Next Steps

* Compare the performance of the tuned Gradient Boosting model against other tuned models on the filtered data to determine the most effective model for this dataset.
* Further investigate the hyperparameters of the Gradient Boosting model, potentially exploring a wider range of values or different tuning methods, if the current performance is not satisfactory.

## Summary:

### Data Analysis Key Findings

*   The best hyperparameters found by GridSearchCV for the Random Forest model on the filtered data are `max_depth`: 10, `min_samples_leaf`: 1, `min_samples_split`: 10, and `n_estimators`: 300.
*   The best cross-validation score obtained during the tuning process was a negative Mean Squared Error of -3151711959694.924.
*   The tuned Random Forest model on the filtered test data achieved a Mean Squared Error of 2781194879149.8667, an R-squared of 0.20003177667298078, and a Mean Absolute Error of 1266268.625150876.
*   Compared to the untuned retrained Random Forest model on filtered data, the tuned model showed a slightly lower R-squared (0.200032 vs 0.218226) and slightly higher MSE (2.78e+12 vs 2.72e+12) and MAE (1.27e+06 vs 1.24e+06).

### Insights or Next Steps

*   While tuning did not significantly improve the Random Forest model's performance on the filtered data in this instance, it's a valuable step to ensure optimal parameters are used.
*   Further investigation into feature engineering or exploring other model types might be beneficial to improve performance on the filtered dataset, given the relatively low R-squared value.

## Summary:

### Data Analysis Key Findings

*   GridSearchCV identified the best hyperparameters for the Gradient Boosting model on the filtered data as: `learning_rate=0.01`, `max_depth=3`, `min_samples_leaf=2`, `min_samples_split=10`, and `n_estimators=200`.
*   The best cross-validation score achieved by GridSearchCV for the Gradient Boosting model was approximately -3.05e+12 (negative Mean Squared Error).
*   When evaluated on the filtered test data, the tuned Gradient Boosting model achieved a Mean Squared Error (MSE) of approximately 2.77e+12, an R-squared of approximately 0.204, and a Mean Absolute Error (MAE) of approximately 1.25e+06.

### Insights or Next Steps

*   Compare the performance of the tuned Gradient Boosting model against other tuned models on the filtered data to determine the most effective model for this dataset.
*   Further investigate the hyperparameters of the Gradient Boosting model, potentially exploring a wider range of values or different tuning methods, if the current performance is not satisfactory.

## Huspris Prediktion i Västra Götalands län

### Rapport:

Analys och prediktion av huspriser i dom kommunerna som tillhör Västra Götalands län
Denna rapport sammanfattar processen för att förbereda data, välja modell, utvärdera prestanda och
föreslå förbättringar för att förutsäga huspriser i Västra Götalands län baserat på tillgängligt dataset.

### 1. Dataförberedelse

Datasetet laddades in och genomgick flera steg för rening och förberedelse:

- Kolumner togs bort: Onödiga kolumner som ad_id, date_published och coordenates togs bort.
- Filtrering på hustyp: Endast hus (typology_HOUSE) behölls för analysen.
- Namnbyte av kolumner: Kolumnerna döptes om till svenska namn för tydlighet: land_area_sqm till tomtyta,
  living_area_sqm till boyta, number_rooms till rum, typology_HOUSE till hus, asking_price_sek till utgångspris
  och sqm_price_sek till pris_sqm.
- Extrahering och filtrering av kommun: Kolumnerna address och location slogs samman. Kommunnamnet extraherades
  från kolumnen location, och datasetet filtrerades sedan för att endast inkludera kommuner inom Västra Götalands län
  (baserat på en angiven lista).
- Hantering av saknade värden: Rader med saknade värden eller värdet noll i tomtyta och boyta togs bort.
  Rader med värdet mindre än 1 i rum togs också bort.
- Hantering av extremvärden (Outliers): IQR-metoden (Interquartile Range) tillämpades för att identifiera
  och ta bort extremvärden i de numeriska kolumnerna tomtyta, boyta, rum och utgångspris.
- Imputering av saknade värden: Eventuella kvarvarande saknade numeriska värden fylldes i med medelvärdet
  för respektive kolumn.
- Feature Engineering: En interaktionskolumn, boyta_rum_interaktion, skapades genom att multiplicera boyta med rum.
- One-Hot Encoding: Den kategoriska kolumnen municipality omvandlades till numeriska kolumner med one-hot encoding.
- Skalning: Numeriska features skalades med StandardScaler för att normalisera deras värdeintervall.
- Datasetet delades sedan upp i träningsdata (80%) och testdata (20%).

### 2. Modellval och träning

Flera regressionsmodeller testades för att förutsäga utgångspris:

Linjär Regression: En enkel och tolkningsbar modell som fungerar bra om det finns linjära samband i datat.
Random Forest Regressor: En ensemble-modell som bygger på flera beslutsträd och ofta presterar bra på komplexa dataset.
Gradient Boosting Regressor: Ytterligare en kraftfull ensemble-modell som sekventiellt bygger träd för att
korrigera fel från föregående träd.
Modellerna tränades på träningsdatan med standardinställningar för Random Forest och Gradient Boosting.

### 3. Utvärdering av modellprestanda

Modellerna utvärderades på testdatan med hjälp av följande regressionsmetriker:

Mean Squared Error (MSE): Mäter medelkvadratfelet mellan predikterade och faktiska värden.
Lägre värde indikerar bättre prestanda.
R-squared (R²): Mäter hur stor andel av variansen i den beroende variabeln som förklaras av modellen.
Ett värde närmare 1 indikerar bättre förklaringsförmåga.
Mean Absolute Error (MAE): Mäter medelvärdet av de absoluta skillnaderna mellan predikterade och
faktiska värden. Lägre värde indikerar bättre prestanda.

Prestandan för modellerna var som följer:
Modell MSE R-squared MAE
Linjär Regression 1.9396e+12 0.4890 980087.59
Random Forest 2.8845e+12 0.3694 1304926.90
Gradient Boosting 1.8192e+12 0.5207 1021044.00
Notera: Metrikvärdena kan variera något beroende på den exakta datafiltreringen och outlierhanteringen.

Baserat på dessa metriker presterade Gradient Boosting Regressor bäst med högst R-squared (0.5207) och lägst MSE.
Linjär Regression kom på andra plats, medan Random Forest presterade sämst.
En R-squared på runt 0.52 indikerar att modellerna förklarar cirka hälften av variationen i bostadspriserna.
De höga värdena för MSE och MAE (i miljonklassen) tyder på att modellerna har en betydande genomsnittlig
felmarginal i sina prisprediktioner.

### 4. Förbättringsförslag

För att förbättra modellernas prestanda kan följande åtgärder övervägas:

Mer avancerad Feature Engineering:

Slå upp fastigheterna i offentliga fastighetsdatakällor (exempelvis Lantmäteriet eller Booli) för att hämta byggnadsår
och inkludera det som en feature. Eller använda geokoordinaterna för att matcha mot öppna databaser som har byggnadsår
kopplat till plats.
Skapa fler interaktionstermer mellan relevanta numeriska features.
Lägg till polynomfunktioner för att fånga icke-linjära samband.
Inkludera tidsbaserade features om det finns tidsstämplar i datat (t.ex. säsongsvariationer).
Utforska mer detaljerade geografiska features, t.ex. avstånd till centrum, skolor, kommunikationer etc.,
om sådan data finns tillgänglig.

Hyperparameteroptimering: Använd metoder som GridSearchCV eller RandomizedSearchCV för att finjustera hyperparametrarna
för Random Forest och Gradient Boosting modellerna. Detta kan avsevärt förbättra deras prestanda.

Utvärdera andra modeller: Testa andra regressionsmodeller som t.ex. XGBoost, LightGBM, Support Vector Regressor (SVR)
eller enklare neurala nätverk.

Cross-Validation: Använd k-fold cross-validation för en mer robust utvärdering av modellernas prestanda och för att
säkerställa att resultaten inte är beroende av en specifik tränings-/testuppdelning.
Mer sofistikerad Outlierhantering: Utforska alternativa metoder för outlierdetektion och hantering som kan vara mer
lämpliga för specifika features.

Datainsamling: Om möjligt, samla in mer relevant data som kan påverka bostadspriser, t.ex. fastighetens ålder, skick,
renoveringar, närhet till service, brottsstatistik i området, etc.
Genom att implementera dessa förbättringsförslag kan modellernas förmåga att korrekt förutsäga bostadspriser potentiellt ökas.
"""